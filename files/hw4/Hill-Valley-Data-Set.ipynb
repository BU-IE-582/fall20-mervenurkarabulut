{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hill-Valley Data Set\n",
    "\n",
    "**Data Set Information:**\n",
    "\n",
    "Each record represents 100 points on a two-dimensional graph. When plotted in order (from 1 through 100) as the Y coordinate, the points will create either a Hill (a “bump” in the terrain) or a Valley (a “dip” in the terrain).\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "1-100: Labeled “X##”. Floating point values (numeric)\n",
    "\n",
    "101: Labeled “class”. Binary {0, 1} representing {valley, hill}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t606 obs. of  101 variables:\n",
      " $ X1   : num  1317.27 7329.97 809.42 45334.21 1.81 ...\n",
      " $ X2   : num  1315.22 7379.91 809.78 45334.21 1.81 ...\n",
      " $ X3   : num  1312.77 7441.8 810.21 45334.22 1.81 ...\n",
      " $ X4   : num  1309.83 7518.5 810.72 45334.23 1.81 ...\n",
      " $ X5   : num  1306.32 7613.57 811.32 45334.23 1.81 ...\n",
      " $ X6   : num  1302.1 7731.38 812.04 45334.24 1.81 ...\n",
      " $ X7   : num  1297.05 7877.39 812.9 45334.25 1.81 ...\n",
      " $ X8   : num  1290.99 8058.34 813.92 45334.26 1.81 ...\n",
      " $ X9   : num  1283.74 8282.6 815.14 45334.28 1.81 ...\n",
      " $ X10  : num  1275.04 8560.53 816.59 45334.3 1.81 ...\n",
      " $ X11  : num  1264.62 8904.97 818.31 45334.32 1.81 ...\n",
      " $ X12  : num  1252.14 9331.85 820.36 45334.34 1.81 ...\n",
      " $ X13  : num  1237.18 9860.9 822.81 45334.37 1.81 ...\n",
      " $ X14  : num  1219.25 10516.56 825.72 45334.4 1.81 ...\n",
      " $ X15  : num  1197.77 9860.9 829.18 45334.43 1.81 ...\n",
      " $ X16  : num  1172.02 9331.85 833.3 45334.48 1.81 ...\n",
      " $ X17  : num  1141.17 8904.97 838.21 45334.53 1.81 ...\n",
      " $ X18  : num  1104.2 8560.53 844.06 45334.59 1.81 ...\n",
      " $ X19  : num  1059.9 8282.6 851.02 45334.66 1.81 ...\n",
      " $ X20  : num  1006.82 8058.34 859.31 45334.74 1.81 ...\n",
      " $ X21  : num  943.21 7877.39 869.17 45334.84 1.81 ...\n",
      " $ X22  : num  866.98 7731.38 880.92 45334.95 1.81 ...\n",
      " $ X23  : num  943.21 7613.57 894.9 45335.08 1.81 ...\n",
      " $ X24  : num  1006.82 7518.5 911.55 45335.24 1.81 ...\n",
      " $ X25  : num  1059.9 7441.8 931.38 45335.42 1.81 ...\n",
      " $ X26  : num  1104.2 7379.91 954.98 45335.64 1.81 ...\n",
      " $ X27  : num  1141.17 7329.97 983.08 45335.89 1.81 ...\n",
      " $ X28  : num  1172.02 7289.67 1016.53 45336.18 1.81 ...\n",
      " $ X29  : num  1197.77 7257.16 1056.36 45336.53 1.81 ...\n",
      " $ X30  : num  1219.25 7230.92 1103.78 45336.93 1.81 ...\n",
      " $ X31  : num  1237.18 7209.75 1056.36 45337.41 1.81 ...\n",
      " $ X32  : num  1252.14 7192.67 1016.53 45337.97 1.81 ...\n",
      " $ X33  : num  1264.62 7178.89 983.08 45338.62 1.81 ...\n",
      " $ X34  : num  1275.04 7167.77 954.98 45339.39 1.81 ...\n",
      " $ X35  : num  1283.74 7158.79 931.38 45340.29 1.81 ...\n",
      " $ X36  : num  1290.99 7151.55 911.55 45341.34 1.81 ...\n",
      " $ X37  : num  1297.05 7145.71 894.9 45342.58 1.81 ...\n",
      " $ X38  : num  1302.1 7141 880.92 45344.03 1.81 ...\n",
      " $ X39  : num  1306.32 7137.19 869.17 45345.73 1.81 ...\n",
      " $ X40  : num  1309.83 7134.12 859.31 45347.72 1.81 ...\n",
      " $ X41  : num  1312.77 7131.65 851.02 45350.06 1.81 ...\n",
      " $ X42  : num  1315.22 7129.65 844.06 45352.8 1.81 ...\n",
      " $ X43  : num  1317.27 7128.04 838.21 45356.02 1.81 ...\n",
      " $ X44  : num  1318.97 7126.74 833.3 45359.79 1.81 ...\n",
      " $ X45  : num  1320.4 7125.69 829.18 45364.21 1.81 ...\n",
      " $ X46  : num  1321.58 7124.84 825.72 45369.4 1.81 ...\n",
      " $ X47  : num  1322.58 7124.16 822.81 45375.48 1.81 ...\n",
      " $ X48  : num  1323.4 7123.6 820.36 45382.62 1.81 ...\n",
      " $ X49  : num  1324.09 7123.16 818.31 45390.98 1.81 ...\n",
      " $ X50  : num  1324.67 7122.8 816.59 45400.79 1.81 ...\n",
      " $ X51  : num  1325.15 7122.51 815.14 45412.3 1.81 ...\n",
      " $ X52  : num  1325.55 7122.28 813.92 45425.79 1.81 ...\n",
      " $ X53  : num  1325.89 7122.09 812.9 45441.61 1.81 ...\n",
      " $ X54  : num  1326.17 7121.94 812.04 45460.16 1.81 ...\n",
      " $ X55  : num  1326.4 7121.81 811.32 45481.92 1.81 ...\n",
      " $ X56  : num  1326.6 7121.71 810.72 45507.44 1.81 ...\n",
      " $ X57  : num  1326.76 7121.63 810.21 45537.36 1.81 ...\n",
      " $ X58  : num  1326.89 7121.57 809.78 45572.45 1.81 ...\n",
      " $ X59  : num  1327.01 7121.52 809.42 45613.6 1.81 ...\n",
      " $ X60  : num  1327.1 7121.48 809.12 45661.85 1.81 ...\n",
      " $ X61  : num  1327.18 7121.44 808.87 45718.45 1.81 ...\n",
      " $ X62  : num  1327.25 7121.41 808.65 45784.81 1.81 ...\n",
      " $ X63  : num  1327.3 7121.39 808.48 45862.64 1.81 ...\n",
      " $ X64  : num  1327.35 7121.37 808.33 45953.91 1.81 ...\n",
      " $ X65  : num  1327.38 7121.36 808.2 46060.94 1.81 ...\n",
      " $ X66  : num  1327.42 7121.35 808.09 46186.45 1.81 ...\n",
      " $ X67  : num  1327.44 7121.34 808.01 46333.65 1.81 ...\n",
      " $ X68  : num  1327.46 7121.33 807.93 46506.26 1.81 ...\n",
      " $ X69  : num  1327.5 7121.3 807.9 46708.7 1.8 ...\n",
      " $ X70  : num  1327.5 7121.3 807.8 46946.1 1.8 ...\n",
      " $ X71  : num  1327.5 7121.3 807.8 47224.5 1.8 ...\n",
      " $ X72  : num  1327.5 7121.3 807.7 47550.9 1.8 ...\n",
      " $ X73  : num  1327.53 7121.31 807.7 47933.77 1.79 ...\n",
      " $ X74  : num  1327.54 7121.31 807.68 48382.74 1.79 ...\n",
      " $ X75  : num  1327.55 7121.31 807.65 48909.24 1.78 ...\n",
      " $ X76  : num  1327.55 7121.31 807.64 49526.68 1.78 ...\n",
      " $ X77  : num  1327.55 7121.3 807.62 50250.75 1.77 ...\n",
      " $ X78  : num  1327.56 7121.3 807.61 51099.88 1.75 ...\n",
      " $ X79  : num  1327.56 7121.3 807.6 52095.66 1.74 ...\n",
      " $ X80  : num  1327.56 7121.3 807.59 53263.41 1.72 ...\n",
      " $ X81  : num  1327.57 7121.3 807.58 54632.85 1.69 ...\n",
      " $ X82  : num  1327.57 7121.3 807.57 56238.79 1.66 ...\n",
      " $ X83  : num  1327.57 7121.3 807.57 54632.85 1.61 ...\n",
      " $ X84  : num  1327.57 7121.3 807.56 53263.41 1.66 ...\n",
      " $ X85  : num  1327.57 7121.3 807.56 52095.66 1.69 ...\n",
      " $ X86  : num  1327.57 7121.3 807.56 51099.88 1.72 ...\n",
      " $ X87  : num  1327.57 7121.3 807.55 50250.75 1.74 ...\n",
      " $ X88  : num  1327.57 7121.3 807.55 49526.68 1.75 ...\n",
      " $ X89  : num  1327.57 7121.3 807.55 48909.24 1.77 ...\n",
      " $ X90  : num  1327.57 7121.3 807.55 48382.74 1.78 ...\n",
      " $ X91  : num  1327.57 7121.3 807.55 47933.77 1.78 ...\n",
      " $ X92  : num  1327.58 7121.3 807.55 47550.92 1.79 ...\n",
      " $ X93  : num  1327.58 7121.3 807.54 47224.46 1.79 ...\n",
      " $ X94  : num  1327.6 7121.3 807.5 46946.1 1.8 ...\n",
      " $ X95  : num  1327.6 7121.3 807.5 46708.7 1.8 ...\n",
      " $ X96  : num  1327.6 7121.3 807.5 46506.3 1.8 ...\n",
      " $ X97  : num  1327.6 7121.3 807.5 46333.6 1.8 ...\n",
      " $ X98  : num  1327.58 7121.3 807.54 46186.45 1.81 ...\n",
      " $ X99  : num  1327.58 7121.3 807.54 46060.94 1.81 ...\n",
      "  [list output truncated]\n"
     ]
    }
   ],
   "source": [
    "# input reading\n",
    "setwd(\"C:/IE582\")\n",
    "Dataset_2<-read.table(\"Hill_Valley.data\",sep=\",\",header=T)\n",
    "colnames(Dataset_2)[101]=\"Class\"\n",
    "Dataset_2$Class<-as.factor(Dataset_2$Class)\n",
    "str(Dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(105) \n",
    "# select 75% of data as train data\n",
    "sample <- sample.int(n = nrow(Dataset_2), size = floor(.75*nrow(Dataset_2)), replace = F)\n",
    "Dataset_2_train <- Dataset_2[sample, ]\n",
    "Dataset_2_test  <- Dataset_2[-sample, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Regression\n",
    "\n",
    "In the penalized regression approach, cross validation is used on the train data to find the best value of the lambda and it is found after 10 fold cross validation. Then with this lambda a model is built to predict class of the test data instances as \"hill\" or \"valley\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'glmnet' was built under R version 3.6.3\"Loading required package: Matrix\n",
      "Loaded glmnet 4.1\n"
     ]
    }
   ],
   "source": [
    "library(glmnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAMAAACJuGjuAAAANlBMVEUAAABNTU1oaGh8fHyM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\nFiABYgESIBYg4X+xPZi/xd1qgAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_train<-as.matrix(Dataset_2_train[,-101])\n",
    "new_test<-as.matrix(Dataset_2_test[,-101])\n",
    "cross_validation<-cv.glmnet(new_train,as.matrix(Dataset_2_train$Class) ,family=\"binomial\", alpha=1)\n",
    "options(repr.plot.width=5, repr.plot.height=5)\n",
    "plot(cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The best lambda value: \" \"0.000631152887902228\"   \n"
     ]
    }
   ],
   "source": [
    "print(c(\"The best lambda value: \", cross_validation$lambda.min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The best lambda value is found as 0.00063.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Length Class     Mode     \n",
       "a0           1    -none-    numeric  \n",
       "beta       100    dgCMatrix S4       \n",
       "df           1    -none-    numeric  \n",
       "dim          2    -none-    numeric  \n",
       "lambda       1    -none-    numeric  \n",
       "dev.ratio    1    -none-    numeric  \n",
       "nulldev      1    -none-    numeric  \n",
       "npasses      1    -none-    numeric  \n",
       "jerr         1    -none-    numeric  \n",
       "offset       1    -none-    logical  \n",
       "classnames   2    -none-    character\n",
       "call         6    -none-    call     \n",
       "nobs         1    -none-    numeric  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fitting the penalized regression model\n",
    "fit <- glmnet(new_train,as.matrix(Dataset_2_train$Class) ,family=\"binomial\", alpha=1,lambda=cross_validation$lambda.min)\n",
    "# summarize the model\n",
    "summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"confusion matrix for the train data:\"\n",
      "   \n",
      "      0   1\n",
      "  0 115   0\n",
      "  1 111 228\n",
      "[1] \"train error:\"      \"0.244493392070485\"\n"
     ]
    }
   ],
   "source": [
    "# prediction for train data\n",
    "predictions <- predict(fit, new_train, type=\"class\")\n",
    "# accuracy for train data\n",
    "tab<-table(predictions[,1],Dataset_2_train$Class)\n",
    "print(\"confusion matrix for the train data:\")\n",
    "print(tab)\n",
    "train_error<-1-sum(diag(tab)/sum(tab))\n",
    "print(c(\"train error:\",train_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"confusion matrix for the test data:\"\n",
      "   \n",
      "     0  1\n",
      "  0 41  1\n",
      "  1 38 72\n",
      "[1] \"test error:\"       \"0.256578947368421\"\n"
     ]
    }
   ],
   "source": [
    "# prediction for test data\n",
    "predictions <- predict(fit, new_test, type=\"class\")\n",
    "# accuracy for test data\n",
    "tab<-table(predictions[,1],Dataset_2_test$Class)\n",
    "print(\"confusion matrix for the test data:\")\n",
    "print(tab)\n",
    "test_error<-1-sum(diag(tab)/sum(tab))\n",
    "print(c(\"test error:\",test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Penalized regression with lasso penalty gives **train error of 0.244** and **test error of 0.257** for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "In decision tree approach, for **the minimal number of observations per tree leaf** 2,3,4 and 5 are used; for **the complexity parameter** 0.005, 0.01, 0.015 and 0.02 are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: rpart\n",
      "Warning message:\n",
      "\"package 'rpart' was built under R version 3.6.3\"Loading required package: rattle\n",
      "Warning message:\n",
      "\"package 'rattle' was built under R version 3.6.3\"Loading required package: tibble\n",
      "Warning message:\n",
      "\"package 'tibble' was built under R version 3.6.3\"Loading required package: bitops\n",
      "Rattle: A free graphical interface for data science with R.\n",
      "Version 5.4.0 Copyright (c) 2006-2020 Togaware Pty Ltd.\n",
      "Type 'rattle()' to shake, rattle, and roll your data.\n"
     ]
    }
   ],
   "source": [
    "require(rpart)\n",
    "require(rattle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(154)\n",
    "\n",
    "# function for building a decision tree with different parameters of the minimal number of observations per tree leaf and \n",
    "# the complexity parameter\n",
    "\n",
    "decision_tree <- function(i) {\n",
    "    dtree1<-rpart(Class~.,data = Dataset_2_train,control = rpart.control(minbucket = 1+i,cp=0.005*i))\n",
    "    a<-as.data.frame(printcp(dtree1))\n",
    "\n",
    "    # prediction\n",
    "    predict(dtree1,Dataset_2_test)\n",
    "    \n",
    "    # misclassification error for train data\n",
    "    tab1_train<-table(predict(dtree1,Dataset_2_train, type=\"class\"),Dataset_2_train$Class)\n",
    "    print(tab1_train)\n",
    "    error1_train<-1-sum(diag(tab1_train)/sum(tab1_train))\n",
    "    print(c(\"train error:\",error1_train)) \n",
    "  \n",
    "    # misclassification error for test data\n",
    "    tab1_test<-table(predict(dtree1,Dataset_2_test, type=\"class\"),Dataset_2_test$Class)\n",
    "    error1_test<-1-sum(diag(tab1_test)/sum(tab1_test))\n",
    "    print(c(\"test error:\",error1_test))\n",
    "    \n",
    "    options(repr.plot.width=4, repr.plot.height=4)\n",
    "    plotcp(dtree1)\n",
    "    return(c(error1_train,error1_test))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err <- list()\n",
    "test_err <- list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification tree:\n",
      "rpart(formula = Class ~ ., data = Dataset_2_train, control = rpart.control(minbucket = 1 + \n",
      "    i, cp = 0.005 * i))\n",
      "\n",
      "Variables actually used in tree construction:\n",
      " [1] X1  X15 X19 X22 X23 X25 X29 X36 X37 X39 X40 X44 X46 X48 X51 X55 X59 X62 X64\n",
      "[20] X67 X69 X7  X74 X75\n",
      "\n",
      "Root node error: 226/454 = 0.4978\n",
      "\n",
      "n= 454 \n",
      "\n",
      "         CP nsplit rel error  xerror     xstd\n",
      "1 0.0376106      0   1.00000 1.07080 0.047037\n",
      "2 0.0110619      2   0.92478 1.08407 0.046992\n",
      "3 0.0088496     30   0.47788 1.01327 0.047138\n",
      "4 0.0066372     44   0.34071 0.96903 0.047111\n",
      "5 0.0058997     46   0.32743 0.97788 0.047124\n",
      "6 0.0050000     49   0.30973 0.97788 0.047124\n",
      "   \n",
      "      0   1\n",
      "  0 209  53\n",
      "  1  17 175\n",
      "[1] \"train error:\"      \"0.154185022026432\"\n",
      "[1] \"test error:\"       \"0.361842105263158\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHgCAMAAABKCk6nAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err = decision_tree(1)\n",
    "train_err <- append(train_err,err[1])\n",
    "test_err <- append(test_err,err[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification tree:\n",
      "rpart(formula = Class ~ ., data = Dataset_2_train, control = rpart.control(minbucket = 1 + \n",
      "    i, cp = 0.005 * i))\n",
      "\n",
      "Variables actually used in tree construction:\n",
      " [1] X1  X19 X22 X23 X25 X29 X37 X39 X46 X48 X55 X69 X74 X75\n",
      "\n",
      "Root node error: 226/454 = 0.4978\n",
      "\n",
      "n= 454 \n",
      "\n",
      "        CP nsplit rel error xerror     xstd\n",
      "1 0.037611      0   1.00000 1.1150 0.046853\n",
      "2 0.011062      2   0.92478 1.1150 0.046853\n",
      "3 0.010000     30   0.47788 1.0619 0.047063\n",
      "   \n",
      "      0   1\n",
      "  0 183  65\n",
      "  1  43 163\n",
      "[1] \"train error:\"      \"0.237885462555066\"\n",
      "[1] \"test error:\"       \"0.434210526315789\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHgCAMAAABKCk6nAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nZdkV49YAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err = decision_tree(2)\n",
    "train_err <- append(train_err,err[1])\n",
    "test_err <- append(test_err,err[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification tree:\n",
      "rpart(formula = Class ~ ., data = Dataset_2_train, control = rpart.control(minbucket = 1 + \n",
      "    i, cp = 0.005 * i))\n",
      "\n",
      "Variables actually used in tree construction:\n",
      "[1] X1  X69\n",
      "\n",
      "Root node error: 226/454 = 0.4978\n",
      "\n",
      "n= 454 \n",
      "\n",
      "        CP nsplit rel error xerror     xstd\n",
      "1 0.037611      0   1.00000 1.1814 0.046402\n",
      "2 0.015000      2   0.92478 1.1726 0.046475\n",
      "   \n",
      "      0   1\n",
      "  0 217 200\n",
      "  1   9  28\n",
      "[1] \"train error:\"      \"0.460352422907489\"\n",
      "[1] \"test error:\"       \"0.480263157894737\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHgCAMAAABKCk6nAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAOK0lEQVR4nO2diXaqMBRFE0BEBfz/v60EB+oMuUQ87r1WX3mUNJRtRuDG7UEa\n9+kTgHlBsDgIFgfB4iBYHASLoy/Yvf0ntivnyuP2dqazSQ6CLxTOuXW/mclcF5k/xADnmsvm\nJ0/EEpk/xICBVQQvnm3uXN61pJ0rd+Tw3ypzvhoeeNiRVfvTMf2+46ZzbeaK/4lu0i8cVcFV\n76u6FlyE7/nlwPy0467gInS7Bolu0i8dVcHe1fv9xmWD2nZ18LJ1ebtvc3fuJG+cr/e1d5v9\nnSradUfvh4lu0i8eVcHu7OCkLe/KXeE6Y22odwNFOG4byuQdwbv+mHOim/SLR1Vweahe67rb\nOmrLjw7dsCo+/zR8uyP4+J9zopv0i+d7znQka3/Q4JuTpdytum8IFmJbZqc2uPHHKaobM+8J\nvj78i/i6Ex5FX+gufo9N7oBTG1zsHwseJLpJv3hUBWddv/jUiz777TvNhzHUuZP0sBfdXHYM\nEt2kXzyqgjd9U7kLlgYtZz/s9ec5ycs4+J/g7HDMsAN+TnSTfumoCu5nsrpRzn/B3UyUWw39\nVL6fyfoneJcNBQ8T3aRfOLKCoQfB4iBYHASLg2BxECwOgsVBsDgIFgfB4iBYHASLg2BxECwO\ngsVBsDgIFgfB4iBYHASLg2BxECwOgsVBsDgIFgfB4iBYHASLg2BxECwOgsVBsDgIFiet4IrP\n0yi6CNarOupXJL3i9fdFIfosPgSeiDKc8orXHsGjKLvobWVcSJ+EV7xyOYJH4UNkzLiLlvCK\nu/ILA8UtgC7eT0Rqq9N4Tf2NkQA/T+mi4o+nveIIHsvmshDMNBC8bKrCn1aCmQaCF88qqo5G\n8OJpo3pZCF4+UVcNwQumHwc3XVDkySB4wYSZrLagDZbFx6/ShOBFU56DWU+FKy4OgsVBsDgI\nFgfB4iBYHASLg2BxECwOgsVBsDgIFgfB4iBYHASLg2BxEgvm8zSayEuG4KWDYHEQLA6CxUGw\nOAgWB8HiIFgcBIuDYHEQLM7yBTswY8LVtxf6gSx+BQSLg2BxECwOgsVBsDgIFgfB4iBYHASL\ng2BxECwOgsVBsDgIFgfB4iBYHASLg2BxECwOgsVBsDgIFgfB4iBYHASLg2BxECwOgsVBsDgI\nFmc5giNfaoX7LEdw4ix+BQSLg2BxECwOgsVBsDgIFgfB4iBYHASLEym4KM3O5FEWEEWk4Jlm\njRFsRqTgzLVmp/IgC4giUnBb5Duzc7mfBUQRXUXPcocPwWYgWByGSeIgWJxowZv8UD0XG6PT\nuZsFRBArOD+2wLnVCd1mATFECq6c3x6+bb2r3k1eZYcSvzU/K7hP9ERHHb7XLnudLiQ8Fvnn\nU5wINsNqqvKNYVI4pHRlu9835fMSj2AzzEqwf52uS+j7uc32eYlHsBkJ2+Ag+FTSn5d4BJuR\nsBcdnK5Ogp+WeASbET8OLt4dBx8OW1db1x3als97WQg2I+FM1mDS2jn/9DYjgs1I+URHXVdV\nUYSuVvn8NjKCzeCJDnF4okOclE90tKtDb/s4SckwKREJb/i3PhxXHBManxXcJ6HgMD3ZVj7v\nExqfFdwn4TDJ9wkbnzUITkbCYdLJaZvnCE5GwmHSpced5QhORcJhUuVWx63G5QhORMphUnm2\nur3TKSOM0iwkfS66Lk5bzYoSnAYefBeH56LFQbA4EYLd1PHS6+MRbEa04KMpBC+UTwl+LwuI\nBsHiIFgcBIuDYHEQLE6U4HH3B0Ycj2AzEgquEPwBUk5V1v7dOAAINiPpXHT94r1vgyzgirQ3\nG6rj68QzZgH/Wf7dJGRHgWBxECwOgsVBsDgIFida8LboJqWKxuh8brNAcBSxgvN+1tF5U8Ph\nV/I19csNtydc/cF25fK2+02X11JMoARHEXn5hkm6wHXhvsF8twsRPBpDwae6AMFLwlBwdizB\n70SbnZgFgkdjKPjYBo+JFz02CwSPxlDwvpg94juCR2MpOIyDZ12zAcGjMRU8CwiOwlDwLHHu\nEByJ5TApf7G8xjQQHIXpMMm50n55SgRHYdkGN+uD42xtXFUjOArjTlZTemdcVSM4CvtedDVj\nEBYEj8a6BIdaOnok/Oi1BwSPxrwN9qXt/X5KcBzGvegVveiFYToONp6kvJPFHBlow0yWOEaC\n+5v9s4cyRPBoECwOd5PEQbA4xg/dBfzr9YOnZmH5i3+DGQQ3tMELwkjw9t/k4lxPVRLffzxW\nJTgb+jWdzjpnEd69QPFI5miDbXGDDUctPZbv6UW7wRe8zRyCd8W9vVP5J/gulrnJYSm4nHUm\n63EJvq+dD0PAUPDF7zyP7Ji0wT/3YTAU7N1mn7umyZ1IL1rhw/Dv1Ax60etD6a1tX076jnHw\nMj8MV2XCQPC2e7OQmaznpPswXLVqkYKLQxXduGy/Q7AdUR+G635ppOBtl0UIxPJ2jI4qc654\n0SX7bcFjeKjdSPChAT78s3LvxAnuP255fwrPj0fwZIxL8Kh0XcLSlW33MsTziAAIno5tGzwq\nXZfQ90uGt8/vPiF4Ona96LF9v3/ReJ4fj+AYnNE4eJLg08rf7ukTIAiO4kN3kw6d53W1dd2T\n8m35vJeF4Cg+JvjSh3f+6SPzCI7CVPCIaLN1XVVFEbpa5fNXIhAchaXgmaLNPsoO3sFQMNFm\nl4jp7UKizS4PQ8FEm10ihoKJNrtE7Nvgt6LNjpgYQXAUlr3oEdFmJy4vi+DRmI+D34w2O215\nWQSP5kMzWROXl0XwaD4m+Onyso/qbwSPZg7B9SxvNjzMDp5hJXiXHzpXoUjWBePgBWEkeNfX\npvW+KV4+ZDUxi+tteAsjwXkntXR59yJ4YRswC8FRGAnua2XnvCsed50e/hLGwfNhLHjSy/0I\nnhFjwfEn9DCL6214CwSLg2BxzASPe2x2QhbX2/AWCBbnc3PRU7JA8Gg+JHjiDX8YzYcET7zh\nD6P5VBU97YY/jOa7bvjDaJZ5w98oC/i2XjSMBsHiIPiHWP5MFkSBYHGoosVBsDhWgueJ+A7R\nxAqeN+I7RBMpeO6I7xBLpOC5I75DLJGC54/4DnEYCJ454jtEESk4QcR3iCJS8ISI72OzgChi\nh0kjIr5PzQJiYCZLnEjBtm+N3s0CoojtReemExz3soAoIgV3a0SXpnMcN1lAFLFtcLM+OM7W\nxlU1gs0w6GQ1pXfGVTWCzbDpRVcGd5Nmezzkt7EowaGWfiuY4dQsYDImbbAvTQP6I9gQg170\nil70gokeB5tWzfeygCiYyRLHohdt3+lFsBkIFgfB4iBYHASLg2BxzJ7o4JmsZRIheKh053mq\ncpnEvB/sz3OUq7dWxhqfBUQTITg/PSt7KL5vrGw3JQuIJqYNrnwoxF3x5W7SUonqZLXFoRCb\nF989gg2J7EWvuxVXzE7mbhYQRZTgJg8l2JvfMkSwGVFtsDu1wcbLJiHYjvS96Co7fCBePIOJ\nYDMSjoP7Gc28f2zy+ctqCDYj4UxWEFy6su0epX5e5BFsRsK56CDYu9Bcty4zPiu4T8LXR4Pg\n050nQvonIrXg1UmwnyMLuCGp4GJdbV03aG7L570sBJuRVPD5vaPDAPrpyBnBZqQM4VDXVVUU\noatVPp8ZQbAZxOgQB8HiJBW8WxehFS5eRX1AsBkJBbfZ4A3v5zObCDYjoeDS+U2/Llaz9QyT\nEpFQsB8se1Yz0ZGI1DNZd/9jlgXcQAkWJ20bvO2fvqQNTkfKYVI+6EVnN1NZhFGahbTj4DKM\ng32xZhycCmayxEGwOAgW51OCGQcnAsHiUEWLg2BxECwOgsVBsDgIFucDz0W/cT8BwWYkFFwh\n+AMkffDdvxtMC8FmJG2D63cXKUWwGWk7WdXgqZ2ZsoD/0IsWB8HiIFgcBIuDYHEQLA6CxUGw\nOAgWB8HiIFichQoGMyZcfXuhC8pOgchLhuClg2BxECwOgsVBsDgIFgfB4iBYHASLg2BxECzO\ndwmG1CBYHASLg2BxECwOgsVBsDgIFgfB4iBYHASLg2BxECwOgsVBsDgIFieN4Ou3ptqVezdO\n3g9ReufL9v6OKhv+bPe+ts8I7tZMWyfJ+YvoFwvM7u4ow6Y/Gm790gU71yTJ95vYOV/va+92\nd3bUbtV2sSNX/U+KEe+Rfkpwkmy/itJtD/9uLjXbYEfRX6/jZduMeVE4oeBDsS2cX5/eCE+S\n8RdRhFqtdsXDHUfBjcuXKti70PYi+B7uXym9t6N1IUR37pqlCs7bQzuS7ami7/FacBXq7LXb\njLl+SQXvLptJsv0qXgpufFdZhyp7qYKvNmHIK8Ftv4ZC1o2VEPyF+GvBVzvyMCBehWoawV9I\n32lurnvRxx1Nloepg9ERlRC8FNahbG4vU7jDHVt3XOMGwV/Ls5msxv1fw4gq+hvJQsEMJvvr\nc9mxuiq2CP5G2nDzKGz21+ey47peXpxg+BgIFgfB4iBYHASLg2BxECwOgsVBsDgIFgfB4iBY\nHASLg2BxECwOgsVBsDgIFgfB4iBYHASLg2BxECwOgsVBsDgIFgfB4iBYHASLg2BxECwOgsVB\nsDgIFgfB4iBYHASL8+uCS+9CCDnn9uUpxo0UPy44Py2F4Nz6HKVKit8WvOlCWK+6YIHuGFZu\n8+lTsua3BRddnMDW+U5wHxiyeJnmy/htwU/Cq6sg9weNAsHiIFicfNAGd2F8t6elp3T4bcFV\n14suh73o7adPyZrfFjwcB4dNuU70rwvuVn0sjjNZhcuqT5+OPb8u+IRe7+qI6t81FgSLg2Bx\nEAzfCYLFQbA4CBYHweIgWBwEi4NgcRAsDoLFQbA4CBYHweIgWBwEi4NgcRAsDoLFQbA4CBYH\nweIgWBwEi4NgcRAsDoLFQbA4f5wwxrsLDK16AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err = decision_tree(3)\n",
    "train_err <- append(train_err,err[1])\n",
    "test_err <- append(test_err,err[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification tree:\n",
      "rpart(formula = Class ~ ., data = Dataset_2_train, control = rpart.control(minbucket = 1 + \n",
      "    i, cp = 0.005 * i))\n",
      "\n",
      "Variables actually used in tree construction:\n",
      "[1] X1  X69\n",
      "\n",
      "Root node error: 226/454 = 0.4978\n",
      "\n",
      "n= 454 \n",
      "\n",
      "        CP nsplit rel error xerror     xstd\n",
      "1 0.037611      0   1.00000 1.1504 0.046639\n",
      "2 0.020000      2   0.92478 1.1504 0.046639\n",
      "   \n",
      "      0   1\n",
      "  0 217 200\n",
      "  1   9  28\n",
      "[1] \"train error:\"      \"0.460352422907489\"\n",
      "[1] \"test error:\"       \"0.480263157894737\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHgCAMAAABKCk6nAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nPyu4vIxDSf/UAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err = decision_tree(4)\n",
    "train_err <- append(train_err,err[1])\n",
    "test_err <- append(test_err,err[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li>0.154185022026432</li>\n",
       "\t<li>0.237885462555066</li>\n",
       "\t<li>0.460352422907489</li>\n",
       "\t<li>0.460352422907489</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item 0.154185022026432\n",
       "\\item 0.237885462555066\n",
       "\\item 0.460352422907489\n",
       "\\item 0.460352422907489\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 0.154185022026432\n",
       "2. 0.237885462555066\n",
       "3. 0.460352422907489\n",
       "4. 0.460352422907489\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] 0.154185\n",
       "\n",
       "[[2]]\n",
       "[1] 0.2378855\n",
       "\n",
       "[[3]]\n",
       "[1] 0.4603524\n",
       "\n",
       "[[4]]\n",
       "[1] 0.4603524\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li>0.361842105263158</li>\n",
       "\t<li>0.434210526315789</li>\n",
       "\t<li>0.480263157894737</li>\n",
       "\t<li>0.480263157894737</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item 0.361842105263158\n",
       "\\item 0.434210526315789\n",
       "\\item 0.480263157894737\n",
       "\\item 0.480263157894737\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 0.361842105263158\n",
       "2. 0.434210526315789\n",
       "3. 0.480263157894737\n",
       "4. 0.480263157894737\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] 0.3618421\n",
       "\n",
       "[[2]]\n",
       "[1] 0.4342105\n",
       "\n",
       "[[3]]\n",
       "[1] 0.4802632\n",
       "\n",
       "[[4]]\n",
       "[1] 0.4802632\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For both train and test data the decision tree with **the minimal number of observations per tree leaf as 2** and **the complexity parameter as 0.0005** has a better performance when compared with train and test errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "In random forest approach, J trees are fit to bootstrap samples using a random sample of m features on which to split each\n",
    "node. I used (12,14,16,18,20) as m in each iteration to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: randomForest\n",
      "Warning message:\n",
      "\"package 'randomForest' was built under R version 3.6.3\"randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "The following object is masked from 'package:rattle':\n",
      "\n",
      "    importance\n",
      "\n",
      "Warning message:\n",
      "\"package 'cowplot' was built under R version 3.6.3\""
     ]
    }
   ],
   "source": [
    "require(randomForest)\n",
    "library(cowplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(105)\n",
    "\n",
    "random_forest <- function(i){\n",
    "    model<-randomForest(Class~.,data=Dataset_2_train,proximity=TRUE,mtry=10+2*i)\n",
    "    print(model)\n",
    "    \n",
    "    # misclassification error for train data\n",
    "    tab1_train<-table(predict(model,Dataset_2_train, type=\"class\"),Dataset_2_train$Class)\n",
    "    print(tab1_train)\n",
    "    error1_train<-1-sum(diag(tab1_train)/sum(tab1_train))\n",
    "    print(c(\"train error:\",error1_train)) \n",
    "  \n",
    "    # misclassification error for test data\n",
    "    tab1_test<-table(predict(model,Dataset_2_test, type=\"class\"),Dataset_2_test$Class)\n",
    "    error1_test<-1-sum(diag(tab1_test)/sum(tab1_test))\n",
    "    print(c(\"test error:\",error1_test))\n",
    "    \n",
    "    return(error1_test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "err <- list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      " randomForest(formula = Class ~ ., data = Dataset_2_train, proximity = TRUE,      mtry = 10 + 2 * i) \n",
      "               Type of random forest: classification\n",
      "                     Number of trees: 500\n",
      "No. of variables tried at each split: 12\n",
      "\n",
      "        OOB estimate of  error rate: 46.48%\n",
      "Confusion matrix:\n",
      "    0   1 class.error\n",
      "0 121 105   0.4646018\n",
      "1 106 122   0.4649123\n",
      "   \n",
      "      0   1\n",
      "  0 226   0\n",
      "  1   0 228\n",
      "[1] \"train error:\" \"0\"           \n",
      "[1] \"test error:\"       \"0.368421052631579\"\n"
     ]
    }
   ],
   "source": [
    "e = random_forest(1)\n",
    "err = append(err,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      " randomForest(formula = Class ~ ., data = Dataset_2_train, proximity = TRUE,      mtry = 10 + 2 * i) \n",
      "               Type of random forest: classification\n",
      "                     Number of trees: 500\n",
      "No. of variables tried at each split: 14\n",
      "\n",
      "        OOB estimate of  error rate: 48.02%\n",
      "Confusion matrix:\n",
      "    0   1 class.error\n",
      "0 112 114   0.5044248\n",
      "1 104 124   0.4561404\n",
      "   \n",
      "      0   1\n",
      "  0 226   0\n",
      "  1   0 228\n",
      "[1] \"train error:\" \"0\"           \n",
      "[1] \"test error:\"       \"0.401315789473684\"\n"
     ]
    }
   ],
   "source": [
    "e = random_forest(2)\n",
    "err = append(err,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      " randomForest(formula = Class ~ ., data = Dataset_2_train, proximity = TRUE,      mtry = 10 + 2 * i) \n",
      "               Type of random forest: classification\n",
      "                     Number of trees: 500\n",
      "No. of variables tried at each split: 16\n",
      "\n",
      "        OOB estimate of  error rate: 48.02%\n",
      "Confusion matrix:\n",
      "    0   1 class.error\n",
      "0 114 112   0.4955752\n",
      "1 106 122   0.4649123\n",
      "   \n",
      "      0   1\n",
      "  0 226   0\n",
      "  1   0 228\n",
      "[1] \"train error:\" \"0\"           \n",
      "[1] \"test error:\" \"0.375\"      \n"
     ]
    }
   ],
   "source": [
    "e = random_forest(3)\n",
    "err = append(err,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      " randomForest(formula = Class ~ ., data = Dataset_2_train, proximity = TRUE,      mtry = 10 + 2 * i) \n",
      "               Type of random forest: classification\n",
      "                     Number of trees: 500\n",
      "No. of variables tried at each split: 18\n",
      "\n",
      "        OOB estimate of  error rate: 47.14%\n",
      "Confusion matrix:\n",
      "    0   1 class.error\n",
      "0 118 108   0.4778761\n",
      "1 106 122   0.4649123\n",
      "   \n",
      "      0   1\n",
      "  0 226   0\n",
      "  1   0 228\n",
      "[1] \"train error:\" \"0\"           \n",
      "[1] \"test error:\"       \"0.394736842105263\"\n"
     ]
    }
   ],
   "source": [
    "e = random_forest(4)\n",
    "err = append(err,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      " randomForest(formula = Class ~ ., data = Dataset_2_train, proximity = TRUE,      mtry = 10 + 2 * i) \n",
      "               Type of random forest: classification\n",
      "                     Number of trees: 500\n",
      "No. of variables tried at each split: 20\n",
      "\n",
      "        OOB estimate of  error rate: 48.24%\n",
      "Confusion matrix:\n",
      "    0   1 class.error\n",
      "0 113 113   0.5000000\n",
      "1 106 122   0.4649123\n",
      "   \n",
      "      0   1\n",
      "  0 226   0\n",
      "  1   0 228\n",
      "[1] \"train error:\" \"0\"           \n",
      "[1] \"test error:\"       \"0.394736842105263\"\n"
     ]
    }
   ],
   "source": [
    "e = random_forest(5)\n",
    "err = append(err,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li>0.368421052631579</li>\n",
       "\t<li>0.401315789473684</li>\n",
       "\t<li>0.375</li>\n",
       "\t<li>0.394736842105263</li>\n",
       "\t<li>0.394736842105263</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item 0.368421052631579\n",
       "\\item 0.401315789473684\n",
       "\\item 0.375\n",
       "\\item 0.394736842105263\n",
       "\\item 0.394736842105263\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 0.368421052631579\n",
       "2. 0.401315789473684\n",
       "3. 0.375\n",
       "4. 0.394736842105263\n",
       "5. 0.394736842105263\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] 0.3684211\n",
       "\n",
       "[[2]]\n",
       "[1] 0.4013158\n",
       "\n",
       "[[3]]\n",
       "[1] 0.375\n",
       "\n",
       "[[4]]\n",
       "[1] 0.3947368\n",
       "\n",
       "[[5]]\n",
       "[1] 0.3947368\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing the test error rates, the best model is found with m=12 but all the train error rates are 0, this may indicate to an overfitting issue.\n",
    "\n",
    "- With the best decision tree, test error is 0.3618 and in this approach with the best model, test error is 0.3684. Thus, decision tree performs better than random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting (SGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'caret' was built under R version 3.6.3\"Loading required package: lattice\n",
      "Warning message:\n",
      "\"package 'lattice' was built under R version 3.6.3\"Loading required package: ggplot2\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.3\"\n",
      "Attaching package: 'ggplot2'\n",
      "\n",
      "The following object is masked from 'package:randomForest':\n",
      "\n",
      "    margin\n",
      "\n",
      "Loading required package: gbm\n",
      "Warning message:\n",
      "\"package 'gbm' was built under R version 3.6.3\"Loaded gbm 2.1.8\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "require(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"Setting `distribution = \"multinomial\"` is ill-advised as it is currently broken. It exists only for backwards compatibility. Use at your own risk.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>var</th><th scope=col>rel.inf</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>X1       </td><td>19.615344</td></tr>\n",
       "\t<tr><th scope=row>X36</th><td>X36      </td><td> 5.168676</td></tr>\n",
       "\t<tr><th scope=row>X38</th><td>X38      </td><td> 4.630071</td></tr>\n",
       "\t<tr><th scope=row>X21</th><td>X21      </td><td> 3.941656</td></tr>\n",
       "\t<tr><th scope=row>X19</th><td>X19      </td><td> 3.781811</td></tr>\n",
       "\t<tr><th scope=row>X48</th><td>X48      </td><td> 3.529660</td></tr>\n",
       "\t<tr><th scope=row>X77</th><td>X77      </td><td> 2.908769</td></tr>\n",
       "\t<tr><th scope=row>X72</th><td>X72      </td><td> 2.637949</td></tr>\n",
       "\t<tr><th scope=row>X39</th><td>X39      </td><td> 2.406402</td></tr>\n",
       "\t<tr><th scope=row>X11</th><td>X11      </td><td> 2.347055</td></tr>\n",
       "\t<tr><th scope=row>X13</th><td>X13      </td><td> 2.221221</td></tr>\n",
       "\t<tr><th scope=row>X47</th><td>X47      </td><td> 1.857345</td></tr>\n",
       "\t<tr><th scope=row>X20</th><td>X20      </td><td> 1.805290</td></tr>\n",
       "\t<tr><th scope=row>X45</th><td>X45      </td><td> 1.798545</td></tr>\n",
       "\t<tr><th scope=row>X14</th><td>X14      </td><td> 1.779717</td></tr>\n",
       "\t<tr><th scope=row>X23</th><td>X23      </td><td> 1.764803</td></tr>\n",
       "\t<tr><th scope=row>X40</th><td>X40      </td><td> 1.704617</td></tr>\n",
       "\t<tr><th scope=row>X73</th><td>X73      </td><td> 1.539889</td></tr>\n",
       "\t<tr><th scope=row>X75</th><td>X75      </td><td> 1.515656</td></tr>\n",
       "\t<tr><th scope=row>X78</th><td>X78      </td><td> 1.493643</td></tr>\n",
       "\t<tr><th scope=row>X66</th><td>X66      </td><td> 1.426494</td></tr>\n",
       "\t<tr><th scope=row>X42</th><td>X42      </td><td> 1.305018</td></tr>\n",
       "\t<tr><th scope=row>X34</th><td>X34      </td><td> 1.295909</td></tr>\n",
       "\t<tr><th scope=row>X68</th><td>X68      </td><td> 1.199459</td></tr>\n",
       "\t<tr><th scope=row>X46</th><td>X46      </td><td> 1.136107</td></tr>\n",
       "\t<tr><th scope=row>X65</th><td>X65      </td><td> 1.115316</td></tr>\n",
       "\t<tr><th scope=row>X86</th><td>X86      </td><td> 1.101614</td></tr>\n",
       "\t<tr><th scope=row>X69</th><td>X69      </td><td> 1.088810</td></tr>\n",
       "\t<tr><th scope=row>X44</th><td>X44      </td><td> 1.071831</td></tr>\n",
       "\t<tr><th scope=row>X82</th><td>X82      </td><td> 1.049495</td></tr>\n",
       "\t<tr><th scope=row>...</th><td>...</td><td>...</td></tr>\n",
       "\t<tr><th scope=row>X28</th><td>X28 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X29</th><td>X29 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X30</th><td>X30 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X31</th><td>X31 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X33</th><td>X33 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X35</th><td>X35 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X37</th><td>X37 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X51</th><td>X51 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X53</th><td>X53 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X55</th><td>X55 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X56</th><td>X56 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X59</th><td>X59 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X60</th><td>X60 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X62</th><td>X62 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X63</th><td>X63 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X70</th><td>X70 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X80</th><td>X80 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X81</th><td>X81 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X83</th><td>X83 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X85</th><td>X85 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X87</th><td>X87 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X91</th><td>X91 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X93</th><td>X93 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X94</th><td>X94 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X95</th><td>X95 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X96</th><td>X96 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X97</th><td>X97 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X98</th><td>X98 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X99</th><td>X99 </td><td>0   </td></tr>\n",
       "\t<tr><th scope=row>X100</th><td>X100</td><td>0   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & var & rel.inf\\\\\n",
       "\\hline\n",
       "\tX1 & X1        & 19.615344\\\\\n",
       "\tX36 & X36       &  5.168676\\\\\n",
       "\tX38 & X38       &  4.630071\\\\\n",
       "\tX21 & X21       &  3.941656\\\\\n",
       "\tX19 & X19       &  3.781811\\\\\n",
       "\tX48 & X48       &  3.529660\\\\\n",
       "\tX77 & X77       &  2.908769\\\\\n",
       "\tX72 & X72       &  2.637949\\\\\n",
       "\tX39 & X39       &  2.406402\\\\\n",
       "\tX11 & X11       &  2.347055\\\\\n",
       "\tX13 & X13       &  2.221221\\\\\n",
       "\tX47 & X47       &  1.857345\\\\\n",
       "\tX20 & X20       &  1.805290\\\\\n",
       "\tX45 & X45       &  1.798545\\\\\n",
       "\tX14 & X14       &  1.779717\\\\\n",
       "\tX23 & X23       &  1.764803\\\\\n",
       "\tX40 & X40       &  1.704617\\\\\n",
       "\tX73 & X73       &  1.539889\\\\\n",
       "\tX75 & X75       &  1.515656\\\\\n",
       "\tX78 & X78       &  1.493643\\\\\n",
       "\tX66 & X66       &  1.426494\\\\\n",
       "\tX42 & X42       &  1.305018\\\\\n",
       "\tX34 & X34       &  1.295909\\\\\n",
       "\tX68 & X68       &  1.199459\\\\\n",
       "\tX46 & X46       &  1.136107\\\\\n",
       "\tX65 & X65       &  1.115316\\\\\n",
       "\tX86 & X86       &  1.101614\\\\\n",
       "\tX69 & X69       &  1.088810\\\\\n",
       "\tX44 & X44       &  1.071831\\\\\n",
       "\tX82 & X82       &  1.049495\\\\\n",
       "\t... & ... & ...\\\\\n",
       "\tX28 & X28  & 0   \\\\\n",
       "\tX29 & X29  & 0   \\\\\n",
       "\tX30 & X30  & 0   \\\\\n",
       "\tX31 & X31  & 0   \\\\\n",
       "\tX33 & X33  & 0   \\\\\n",
       "\tX35 & X35  & 0   \\\\\n",
       "\tX37 & X37  & 0   \\\\\n",
       "\tX51 & X51  & 0   \\\\\n",
       "\tX53 & X53  & 0   \\\\\n",
       "\tX55 & X55  & 0   \\\\\n",
       "\tX56 & X56  & 0   \\\\\n",
       "\tX59 & X59  & 0   \\\\\n",
       "\tX60 & X60  & 0   \\\\\n",
       "\tX62 & X62  & 0   \\\\\n",
       "\tX63 & X63  & 0   \\\\\n",
       "\tX70 & X70  & 0   \\\\\n",
       "\tX80 & X80  & 0   \\\\\n",
       "\tX81 & X81  & 0   \\\\\n",
       "\tX83 & X83  & 0   \\\\\n",
       "\tX85 & X85  & 0   \\\\\n",
       "\tX87 & X87  & 0   \\\\\n",
       "\tX91 & X91  & 0   \\\\\n",
       "\tX93 & X93  & 0   \\\\\n",
       "\tX94 & X94  & 0   \\\\\n",
       "\tX95 & X95  & 0   \\\\\n",
       "\tX96 & X96  & 0   \\\\\n",
       "\tX97 & X97  & 0   \\\\\n",
       "\tX98 & X98  & 0   \\\\\n",
       "\tX99 & X99  & 0   \\\\\n",
       "\tX100 & X100 & 0   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | var | rel.inf |\n",
       "|---|---|---|\n",
       "| X1 | X1        | 19.615344 |\n",
       "| X36 | X36       |  5.168676 |\n",
       "| X38 | X38       |  4.630071 |\n",
       "| X21 | X21       |  3.941656 |\n",
       "| X19 | X19       |  3.781811 |\n",
       "| X48 | X48       |  3.529660 |\n",
       "| X77 | X77       |  2.908769 |\n",
       "| X72 | X72       |  2.637949 |\n",
       "| X39 | X39       |  2.406402 |\n",
       "| X11 | X11       |  2.347055 |\n",
       "| X13 | X13       |  2.221221 |\n",
       "| X47 | X47       |  1.857345 |\n",
       "| X20 | X20       |  1.805290 |\n",
       "| X45 | X45       |  1.798545 |\n",
       "| X14 | X14       |  1.779717 |\n",
       "| X23 | X23       |  1.764803 |\n",
       "| X40 | X40       |  1.704617 |\n",
       "| X73 | X73       |  1.539889 |\n",
       "| X75 | X75       |  1.515656 |\n",
       "| X78 | X78       |  1.493643 |\n",
       "| X66 | X66       |  1.426494 |\n",
       "| X42 | X42       |  1.305018 |\n",
       "| X34 | X34       |  1.295909 |\n",
       "| X68 | X68       |  1.199459 |\n",
       "| X46 | X46       |  1.136107 |\n",
       "| X65 | X65       |  1.115316 |\n",
       "| X86 | X86       |  1.101614 |\n",
       "| X69 | X69       |  1.088810 |\n",
       "| X44 | X44       |  1.071831 |\n",
       "| X82 | X82       |  1.049495 |\n",
       "| ... | ... | ... |\n",
       "| X28 | X28  | 0    |\n",
       "| X29 | X29  | 0    |\n",
       "| X30 | X30  | 0    |\n",
       "| X31 | X31  | 0    |\n",
       "| X33 | X33  | 0    |\n",
       "| X35 | X35  | 0    |\n",
       "| X37 | X37  | 0    |\n",
       "| X51 | X51  | 0    |\n",
       "| X53 | X53  | 0    |\n",
       "| X55 | X55  | 0    |\n",
       "| X56 | X56  | 0    |\n",
       "| X59 | X59  | 0    |\n",
       "| X60 | X60  | 0    |\n",
       "| X62 | X62  | 0    |\n",
       "| X63 | X63  | 0    |\n",
       "| X70 | X70  | 0    |\n",
       "| X80 | X80  | 0    |\n",
       "| X81 | X81  | 0    |\n",
       "| X83 | X83  | 0    |\n",
       "| X85 | X85  | 0    |\n",
       "| X87 | X87  | 0    |\n",
       "| X91 | X91  | 0    |\n",
       "| X93 | X93  | 0    |\n",
       "| X94 | X94  | 0    |\n",
       "| X95 | X95  | 0    |\n",
       "| X96 | X96  | 0    |\n",
       "| X97 | X97  | 0    |\n",
       "| X98 | X98  | 0    |\n",
       "| X99 | X99  | 0    |\n",
       "| X100 | X100 | 0    |\n",
       "\n"
      ],
      "text/plain": [
       "     var  rel.inf  \n",
       "X1   X1   19.615344\n",
       "X36  X36   5.168676\n",
       "X38  X38   4.630071\n",
       "X21  X21   3.941656\n",
       "X19  X19   3.781811\n",
       "X48  X48   3.529660\n",
       "X77  X77   2.908769\n",
       "X72  X72   2.637949\n",
       "X39  X39   2.406402\n",
       "X11  X11   2.347055\n",
       "X13  X13   2.221221\n",
       "X47  X47   1.857345\n",
       "X20  X20   1.805290\n",
       "X45  X45   1.798545\n",
       "X14  X14   1.779717\n",
       "X23  X23   1.764803\n",
       "X40  X40   1.704617\n",
       "X73  X73   1.539889\n",
       "X75  X75   1.515656\n",
       "X78  X78   1.493643\n",
       "X66  X66   1.426494\n",
       "X42  X42   1.305018\n",
       "X34  X34   1.295909\n",
       "X68  X68   1.199459\n",
       "X46  X46   1.136107\n",
       "X65  X65   1.115316\n",
       "X86  X86   1.101614\n",
       "X69  X69   1.088810\n",
       "X44  X44   1.071831\n",
       "X82  X82   1.049495\n",
       "...  ...  ...      \n",
       "X28  X28  0        \n",
       "X29  X29  0        \n",
       "X30  X30  0        \n",
       "X31  X31  0        \n",
       "X33  X33  0        \n",
       "X35  X35  0        \n",
       "X37  X37  0        \n",
       "X51  X51  0        \n",
       "X53  X53  0        \n",
       "X55  X55  0        \n",
       "X56  X56  0        \n",
       "X59  X59  0        \n",
       "X60  X60  0        \n",
       "X62  X62  0        \n",
       "X63  X63  0        \n",
       "X70  X70  0        \n",
       "X80  X80  0        \n",
       "X81  X81  0        \n",
       "X83  X83  0        \n",
       "X85  X85  0        \n",
       "X87  X87  0        \n",
       "X91  X91  0        \n",
       "X93  X93  0        \n",
       "X94  X94  0        \n",
       "X95  X95  0        \n",
       "X96  X96  0        \n",
       "X97  X97  0        \n",
       "X98  X98  0        \n",
       "X99  X99  0        \n",
       "X100 X100 0        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAHgCAMAAABKCk6nAAAA8FBMVEUAAAAAAP8AA/8ABf8A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\nCBYHweIgWBwEi4NgcRAsDoLF+R+YMDsHTZaMIQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(123)\n",
    "gbm_tree<-gbm(Class~.,data = Dataset_2_train,distribution=\"multinomial\")\n",
    "summary(gbm_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"train error:\"      \"0.314977973568282\"\n"
     ]
    }
   ],
   "source": [
    "p<-as.data.frame(predict(gbm_tree,type = \"response\",n.trees = 100))\n",
    "predictions<-as.data.frame(matrix(nrow = nrow(Dataset_2_train),ncol = 1))\n",
    "for (i in 1:nrow(predictions)){\n",
    "  if (p[i,1]>p[i,2]) {predictions[i,1]=0\n",
    "  }\n",
    "  else {predictions[i,1]=1\n",
    "  }\n",
    "}\n",
    "tab<-table(predictions[,1],Dataset_2_train[,101])\n",
    "train_error<-1-sum(diag(tab)/sum(tab))\n",
    "print(c(\"train error:\",train_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"test error:\"       \"0.407894736842105\"\n"
     ]
    }
   ],
   "source": [
    "p1<-as.data.frame(predict(gbm_tree,Dataset_2_test, type = \"response\",n.trees = 100))\n",
    "predictions<-as.data.frame(matrix(nrow = nrow(Dataset_2_test),ncol = 1))\n",
    "for (i in 1:nrow(predictions)){\n",
    "  if (p1[i,1]>p1[i,2]) {predictions[i,1]=0\n",
    "  }\n",
    "  else {predictions[i,1]=1\n",
    "  }\n",
    "}\n",
    "tab<-table(predictions[,1],Dataset_2_test[,101])\n",
    "test_error<-1-sum(diag(tab)/sum(tab))\n",
    "print(c(\"test error:\",test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test error is found as *0.4079* in this approach.\n",
    "- In the penalized regression with Lasso penalty, test error is *0.2566*.\n",
    "- In the decision tree with the best model, test error is *0.3618* \n",
    "- In the random forest with the best model, test error is *0.3684*.\n",
    "\n",
    "\n",
    "- Thus considering the error rates, **the penalized regression with Lasso penalty performs better than other approaches.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
